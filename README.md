# Malicious-URL-Detector
This Python program detects malicious URLs using rules and machine learning, helping users identify and avoid online threats.

The provided code implements a Malicious URL Detector that classifies URLs as either malicious or non-malicious using a hybrid approach combining rule-based techniques and machine learning. The system allows users to analyze individual URLs or scan files containing multiple URLs. Its flexible design makes it suitable for both quick checks and batch processing. The primary purpose of this Python script is to help family, friends, and internet users determine whether a URL they want to visit or have received is safe. By testing the URL in the script before accessing it in a browser, users can avoid potential threats and ensure safer online interactions.

The program features an expanded user-friendly menu with seven options for URL analysis. For single URL analysis (Option 1), it automatically adds HTTP prefixes if missing, flags less secure links (e.g., http://), and provides color-coded results, with malicious links highlighted in red and safe links in green. For file-based analysis (Option 2), it processes URLs line by line with a progress indicator showing X/Y URLs processed, providing real-time feedback and a summary report upon completion.
The program maintains detailed logs viewable through (Option 3), which displays both malicious and safe URLs with their detection timestamps. A new statistics feature (Option 4) tracks session duration, total URLs analyzed, and provides detection rates with percentage calculations. Users can export their analysis logs to CSV format using (Option 5), with timestamp-based file naming for easy reference.

A new batch analysis feature (Option 6) allows users to input multiple URLs at once, separated by commas, enabling efficient bulk processing of URLs. The program continues to leverage detection and machine learning techniques while maintaining robust error handling for invalid inputs and file-not-found errors.

Each URL check is now enhanced with metadata tracking, including timestamps, detection methods (single check, file scan, or batch analysis), and source information for file scans. Upon exiting (Option 7), the program provides a comprehensive session summary including total duration, URLs analyzed, and detection statistics. The enhanced logging system and additional features maintain the program's core functionality while providing users with more detailed analysis capabilities and better data management options.

To develop the detector, a dataset of example URLs was created, including non-malicious ones (trusted domains like Google, Amazon, and Outlook) and malicious ones (phishing domains and URLs with suspicious patterns). The dataset was built by researching GitHub repositories and other sources of known malicious URLs to ensure a wide variety of patterns for accurate detection.

A Naive Bayes classifier was trained using this data, leveraging textual features such as URL length, domain structure, and character patterns. The code uses the TfidfVectorizer from scikit-learn to transform the URLs into numerical features by extracting character n-grams and calculating their TF-IDF score. Additional numerical features, including URL length, number of dots, digits, and special characters, are extracted using the extract_features method. This method assesses the URL against various requirements to determine if it matches the machine learning model. The goal is to automate the training process, but currently, the script uses prediction to discover the state of unseen URLs. Furthermore, a rule-based system using regular expressions detects unsafe patterns like .exe file extensions, HTTP links, and suspicious TLDs (e.g., .tk).

Ultimately, this system strikes a balance between flexibility, accuracy, and usability, making it a practical tool for detecting unsafe links in both individual and bulk scenarios. However, certain weaknesses exist in the script at this time: the small dataset for training the model, the model's ability to understand unseen URLs, and the potential limitations of the Naive Bayes classifier, which needs to be capable of recognizing URLs not present in the dataset.

By developing this script, I have deepened my understanding of the technical skills required to effectively detect malicious URLs. As I continue my journey to enhance my expertise in cybersecurity, I plan to refine this script by incorporating the latest advancements in technology and updating its modules. This iterative process will ensure the tool remains relevant and robust, reflecting my growth in the field and my commitment to creating reliable cybersecurity solutions.

For this project, I plan to continuously update and integrate new machine learning models to reduce false positives and improve detection accuracy for users. Please feel free to provide feedback or ask questions about the script. Thank you.
